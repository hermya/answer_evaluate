{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Transfer pollen grains from the anther to the stigma of the same flower.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\Heramb/.cache\\torch\\sentence_transformers\\sbert.net_models_bert-base-nli-mean-tokens\\0_BERT were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9691343]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    [my_sentence_embeddings[0]],\n",
    "    sentence_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = [\"Transfer pollen grains from the anther to the stigma of the different flowers.\"]\n",
    "my_sentence_embeddings = model.encode(my_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Heramb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from to transition to be worked on\n",
    "# or transition to be worked on\n",
    "# adjective transition\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('same', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "sentence = \"It was the same\"\n",
    "x = nltk.pos_tag(word_tokenize(sentence))\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in x:\n",
    "    d[i[1]]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x:\n",
    "    d[i[1]].append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': ['transfer', 'pollen', 'anther', 'stigma', 'flower'],\n",
       " 'NNS': ['grains'],\n",
       " 'IN': ['from', 'of'],\n",
       " 'DT': ['the', 'the', 'the'],\n",
       " 'TO': ['to'],\n",
       " 'JJ': ['same'],\n",
       " '.': ['.']}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark and John are sincere employees at Google.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nnoun_adj_pairs = []\\nfor i,token in enumerate(doc):\\n    if token.pos_ not in ('NOUN','PROPN'):\\n        continue\\n    for j in range(i+1,len(doc)):\\n        if doc[j].pos_ == 'ADJ':\\n            noun_adj_pairs.append((token,doc[j]))\\n            break\\nnoun_adj_pairs\\n\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Mark and John are sincere employees at Google.')\n",
    "print()\n",
    "\"\"\"\n",
    "noun_adj_pairs = []\n",
    "for i,token in enumerate(doc):\n",
    "    if token.pos_ not in ('NOUN','PROPN'):\n",
    "        continue\n",
    "    for j in range(i+1,len(doc)):\n",
    "        if doc[j].pos_ == 'ADJ':\n",
    "            noun_adj_pairs.append((token,doc[j]))\n",
    "            break\n",
    "noun_adj_pairs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ,  Apple ,  PROPN ,  NNP ,  nsubj ,  Xxxxx ,  True ,  False\n",
      "is ,  be ,  AUX ,  VBZ ,  aux ,  xx ,  True ,  True\n",
      "looking ,  look ,  VERB ,  VBG ,  ROOT ,  xxxx ,  True ,  False\n",
      "at ,  at ,  ADP ,  IN ,  prep ,  xx ,  True ,  True\n",
      "buying ,  buy ,  VERB ,  VBG ,  pcomp ,  xxxx ,  True ,  False\n",
      "U.K. ,  U.K. ,  PROPN ,  NNP ,  dobj ,  X.X. ,  False ,  False\n",
      "startup ,  startup ,  NOUN ,  NN ,  advcl ,  xxxx ,  True ,  False\n",
      "for ,  for ,  ADP ,  IN ,  prep ,  xxx ,  True ,  True\n",
      "$ ,  $ ,  SYM ,  $ ,  quantmod ,  $ ,  False ,  False\n",
      "1 ,  1 ,  NUM ,  CD ,  compound ,  d ,  False ,  False\n",
      "billion ,  billion ,  NUM ,  CD ,  pobj ,  xxxx ,  True ,  False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,\", \", token.lemma_,\", \", token.pos_,\", \", token.tag_,\", \", token.dep_,\", \",\n",
    "            token.shape_,\", \", token.is_alpha,\", \", token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(flowers, different)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'It occurs in the flowers which are genetically different.')\n",
    "noun_adj_pairs = []\n",
    "for i,token in enumerate(doc):\n",
    "    if token.pos_ not in ('NOUN','PROPN'):\n",
    "        continue\n",
    "    for j in range(i+1,len(doc)):\n",
    "        if doc[j].pos_ == 'ADJ':\n",
    "            noun_adj_pairs.append((token,doc[j]))\n",
    "            break\n",
    "noun_adj_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"Self-Pollination\" : \"This process can take place in the same flower or a different flower of the same plant.\",\n",
    "       \"Cross-Pollination\" : \"This process can take place between two flowers present on different plants.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk =  It\n",
      "chunk =  the flowers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'flowers': []}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('It occurs in the flowers which are genetically identical.')\n",
    "\n",
    "noun_adj_pairs = {}\n",
    "for chunk in doc.noun_chunks:\n",
    "    adj = []\n",
    "    noun = \"\"\n",
    "    for tok in chunk:\n",
    "        if tok.pos_ == \"NOUN\":\n",
    "            noun = tok.text\n",
    "        if tok.pos_ == \"ADJ\":\n",
    "            adj.append(tok.text)\n",
    "    if noun:\n",
    "        if noun not in noun_adj_pairs.keys():\n",
    "            noun_adj_pairs[noun] = []\n",
    "        noun_adj_pairs[noun] += adj\n",
    "    print(\"chunk = \",chunk)\n",
    "\n",
    "# expected output\n",
    "noun_adj_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
